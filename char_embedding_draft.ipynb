{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "from Vocab import Vocab\n",
    "from Tokenize import Tokenize\n",
    "\n",
    "\n",
    "fo='./models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ye\n"
     ]
    }
   ],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab_size):\n",
    "        super(classifier, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=True,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        self.softmax=nn.Softmax()\n",
    "        self.fc=nn.Linear(hidden_dim, vocab_size)\n",
    "        self.fc1=nn.Linear(hidden_dim, 1000)\n",
    "        self.fc2=nn.Linear(1000, vocab_size)\n",
    "        self.activate_func=nn.ReLU()\n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(vocab_size, input_dim)\n",
    "        #loss\n",
    "        self.cost_func = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.label = 0\n",
    "        self.label_embedding = 0\n",
    "        \n",
    "    def order(self, inputs, inputs_len):\n",
    "        \n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, descending=True)\n",
    "        #print(inputs.shape, sort_ids.shape, inputs_len.shape)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len, is_fix_embedding, embedding_update_num):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "        \n",
    "        if is_fix_embedding:\n",
    "            for para in self.embed.parameters():\n",
    "                para.requires_grad=False\n",
    "                #print('ye')\n",
    "            cnt=0\n",
    "            while(cnt<=embedding_update_num):\n",
    "                cnt+=1\n",
    "                rand_range, _ = self.label_embedding.shape\n",
    "                randn = random.randint(0,rand_range-1)\n",
    "                token_idx = self.label[randn]\n",
    "                token_embedding = self.label_embedding[randn]\n",
    "                self.embed.weight.data[token_idx.data]=token_embedding\n",
    "            print(cnt)\n",
    "        else:\n",
    "            for para in self.embed.parameters():\n",
    "                para.requires_grad=True\n",
    "                \n",
    "#         print(self.embed.weight.data[0])\n",
    "#         self.embed.weight.data[0]=torch.LongTensor(([2]*400))\n",
    "#         print(self.embed.weight.data[0])\n",
    "#         print(self.embed.weight)\n",
    "            \n",
    "        in_vec=self.embed(inputs)\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vec, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        print(hn.shape, cn.shape)\n",
    "        \n",
    "        hn = torch.squeeze(hn[-1,:,:])\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids))\n",
    "#         print(in_vec.size())\n",
    "#         print(outputs.size())\n",
    "#         print(hn.size())\n",
    "        self.label_embedding=hn\n",
    "        return self.softmax(self.fc(hn))\n",
    "\n",
    "    def get_loss(self, predicts, labels):\n",
    "        labels=Variable(labels).long()\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "        self.label=labels\n",
    "        return self.cost_func(predicts, labels)\n",
    "        \n",
    "print('ye')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17388]) torch.Size([17388, 17]) torch.Size([17388])\n"
     ]
    }
   ],
   "source": [
    "# fi_path='./vocab.mean.t2s.txt'\n",
    "# vocab=Vocab(fi_path)\n",
    "# vocab.build_vocab()\n",
    "# vocab.print_vocab()\n",
    "# vocab.line_length_statistics(filter_rate=0.8)\n",
    "# tokenize=Tokenize(fi_path, vocab, length_cut=25)\n",
    "# explains_token, labels_token=tokenize.word_tokenize()\n",
    "\n",
    "with open('./tokenize.txt','rb') as f:\n",
    "    tokenize=pickle.load(f)\n",
    "f.close()\n",
    "vocab=tokenize.vocab\n",
    "#hyper par\n",
    "use_cuda=torch.cuda.is_available()\n",
    "hidden_dim=50\n",
    "input_dim=50\n",
    "vocab_size=len(vocab.word2idx)\n",
    "lr=0.003\n",
    "\n",
    "model=classifier(\n",
    "                use_cuda=use_cuda, \n",
    "                hidden_dim=hidden_dim, input_dim=input_dim, \n",
    "                vocab_size=vocab_size\n",
    "                )\n",
    "#load model trained and continue to train the parameters\n",
    "# model_trained = torch.load('./models/lr0.008-in_dim100-loss8.415006-train_acc 0.29.pkl')\n",
    "# model.load_state_dict(model_trained)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "inputs=tokenize.explains_token\n",
    "labels=tokenize.labels_token\n",
    "inputs_len=tokenize.sentence_lengths\n",
    "\n",
    "inputs_=torch.LongTensor(inputs)\n",
    "labels_=torch.LongTensor(labels)\n",
    "inputs_len_=torch.LongTensor(inputs_len)\n",
    "\n",
    "print(labels_.shape, inputs_.shape, inputs_len_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8694, 50]) torch.Size([2, 8694, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/hmx/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterate 1 times,  loss: 8.702859 train_right: 0 train_acc: 0.000000\n",
      "torch.Size([2, 8693, 50]) torch.Size([2, 8693, 50])\n",
      "iterate 2 times,  loss: 8.702858 train_right: 3 train_acc: 0.000345\n",
      "torch.Size([2, 8694, 50]) torch.Size([2, 8694, 50])\n",
      "iterate 3 times,  loss: 8.702858 train_right: 0 train_acc: 0.000000\n",
      "torch.Size([2, 8693, 50]) torch.Size([2, 8693, 50])\n",
      "iterate 4 times,  loss: 8.702857 train_right: 2 train_acc: 0.000230\n",
      "torch.Size([2, 8694, 50]) torch.Size([2, 8694, 50])\n",
      "iterate 5 times,  loss: 8.702858 train_right: 0 train_acc: 0.000000\n",
      "torch.Size([2, 8693, 50]) torch.Size([2, 8693, 50])\n",
      "iterate 6 times,  loss: 8.702857 train_right: 2 train_acc: 0.000230\n",
      "torch.Size([2, 8694, 50]) torch.Size([2, 8694, 50])\n",
      "iterate 7 times,  loss: 8.702857 train_right: 0 train_acc: 0.000000\n",
      "torch.Size([2, 8693, 50]) torch.Size([2, 8693, 50])\n",
      "iterate 8 times,  loss: 8.702857 train_right: 2 train_acc: 0.000230\n",
      "torch.Size([2, 8694, 50]) torch.Size([2, 8694, 50])\n",
      "iterate 9 times,  loss: 8.702857 train_right: 0 train_acc: 0.000000\n",
      "torch.Size([2, 8693, 50]) torch.Size([2, 8693, 50])\n",
      "iterate 10 times,  loss: 8.702856 train_right: 2 train_acc: 0.000230\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function isinstance> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: can't assign Variable to a torch.cuda.FloatTensor using a LongTensor (only torch.cuda.FloatTensor or float are supported)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a683d5406f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpredicts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_len_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpredicts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_len_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-38a92f4d035d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, inputs_len, is_fix_embedding, embedding_update_num)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mtoken_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;31m# Make sure that we're handling unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function isinstance> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "iterate=0\n",
    "iter_n=1500\n",
    "while(iterate<iter_n):\n",
    "    if iterate%2==0:\n",
    "        inputs_=torch.LongTensor(inputs[0:int(len(inputs)/2)])\n",
    "        labels_=torch.LongTensor(labels[0:int(len(inputs)/2)])\n",
    "        inputs_len_=torch.LongTensor(inputs_len[0:int(len(inputs)/2)])\n",
    "    else:\n",
    "        inputs_=torch.LongTensor(inputs[int(len(inputs)/2):-1])\n",
    "        labels_=torch.LongTensor(labels[int(len(inputs)/2):-1])\n",
    "        inputs_len_=torch.LongTensor(inputs_len[int(len(inputs)/2):-1])\n",
    "\n",
    "#     inputs_=torch.LongTensor(inputs[0:1000])\n",
    "#     labels_=torch.LongTensor(labels[0:1000])\n",
    "#     inputs_len_=torch.LongTensor(inputs_len[0:1000])\n",
    "    \n",
    "    iterate+=1\n",
    "    if iterate<=10:\n",
    "        predicts=model(inputs_, inputs_len_,0,1000)\n",
    "    else:\n",
    "        predicts=model(inputs_, inputs_len_,1,2000)\n",
    "    optimizer.zero_grad()\n",
    "    loss=model.get_loss(predicts, labels_)\n",
    "    #optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iterate%1==0:\n",
    "        if use_cuda:\n",
    "            preds=predicts.data.cpu()\n",
    "        else:\n",
    "            preds=predicts.data\n",
    "        (pred, idxs) = torch.max(preds, 1)\n",
    "        \n",
    "        print('iterate %s times, '%iterate, \n",
    "              'loss: %5.6f' %loss.data[0], \n",
    "              'train_right: %s' %sum(labels_==idxs),\n",
    "              'train_acc: %5.6f' %(sum(labels_==idxs)/len(preds))\n",
    "             )\n",
    "        \n",
    "    #\n",
    "    if iterate%100==0:\n",
    "        torch.save(model.state_dict(), \n",
    "                   fo+'lr{}-in_dim{}-loss{:5.6f}-train_acc{:5.2f}.pkl'.format(lr, input_dim, loss.data[0],\n",
    "                                                                             sum(labels_==idxs)/len(preds))\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
